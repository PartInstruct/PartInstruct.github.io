<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PartInstruct is concerned with part-level instruction following for fine-grained robot manipulation">
  <meta name="keywords" content="PartInstruct, Manipulate Anything">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-96FM6LQE7G"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-96FM6LQE7G');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/partinstruct.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation</h1>
          <div class="is-size-5 publication-authors">

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="about:blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="about:blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="about:blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="about:blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="about:blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/figure_intro.png"
           class="teaser-image"
           alt="PartInstruct teaser image"/>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf"></span>An example fine-grained robot manipulation task in <strong>PartInstruct</strong>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels.
          </p>
          <p>
            In this work, we introduce PartInstruct, the first large-scale benchmark for both training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. Our training set consists of over 10,000 expert demonstrations synthesized in a 3D simulator, where each demonstration is paired with a high-level task instruction, a chain of basic part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks.
          </p>
          <p>
            We evaluated several state-of-the-art robot manipulation approaches including end-to-end vision-language policy learning and bi-level planning models on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts and predict actions in 3D space, and face challenges when manipulating object parts in long-horizon tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="demo_1" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/demo_episode1_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="demo_2" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode772_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="demo_3" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode1215_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="demo_4" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode100342_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="demo_5" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode100663_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="demo_6" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode170165_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="demo_7" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/episode180919_converted.mp4"
                    type="video/mp4">
          </video>
        </div>
        <!-- <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div> -->
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered is-align-top">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3 has-text-centered">Object Categories</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                The following star plot shows the relative distribution of the number of episodes in each object category in the dataset.
              </p>
              <img src="./static/images/figure_obj_distribution-1.png"
                   class="image"
                   alt="Distribution of object category episodes"/>
            </div>

          </div>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3 has-text-centered">Object Parts</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                The following graphs show annotated parts grouped by object categories. Spatial part names are highlighted in light gray to distinguish them from semantic part names.
              </p>
              <img src="./static/images/figure_heatmap.png"
              class="image"
              alt="Part counts among skills"/>
            </div>

          </div>
        </div>
      </div>
    </div>
    <!--/ Matting. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Multimodal Observations in PartGym</h2>
              <div class="column content">
              <p>
                PartGym supports multimodal observations, including RGB images, depth maps, and scene point clouds (PCDs). It also provides object and part annotations, including object segmentations, 2D part segmentation for each object part (part mask), and 3D part segmentations on point clouds (part PCDs) for each object.
              </p>
              <img src="./static/images/figure_vision_modalities_simplified.png"
              class="image"
              alt="Part counts among skills"/>
            </div>

          </div>
        </div>
      </div>
    </div>

    <!-- Animation. -->
<section class="section">
  <div class="container is-max-desktop"> <!-- Ensure the container class is set for proper spacing -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Interactive Demo</h2>      
        <div class="content has-text-justified">
          <p>
            Here we provides a interactive demo to animate the task: "Touch the left of the bucket and then push it to the right". 
            You can use the slider here to see every timestep between the initial and final state.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
        </div>
      </div>
    </div>

      <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>    
   
      
        <div class="content has-text-justified">
          <p>
            To achieve general-purpose robot manipulation, there have been two common types of approaches: (1) end-to-end policy learning that directly maps observation and instruction to actions and (2) bi-level planning that first generates high-level plans (typically subgoals), then compute and execute the low-level action plans to achieve the subgoals. In our benchmark, we evaluate both types of approaches.
          </p>
              <figure>
                <img src="./static/images/figure_bar_plot_with_background_new-1.png"
                     class="image"
                     alt="Part counts among skills" style="max-width: 85%;"/>
                <figcaption>Success Rates of all baselines. The left group represents end-to-end learning policies, while the right group corresponds to bi-level planning models.</figcaption>
              </figure>  
        </div>
        <h3 class="title is-4">End-to-End Policy Learning</h3>      
        <div class="content has-text-justified">
          <p>
            We trained the baselines DP, DP3, Act3D, and RVT2 from scratch and fine-tuned the pretrained baseline Octo on our training data. 
          </p>
        </div>
        <h3 class="title is-4">Bi-level Planning</h3>      
        <div class="content has-text-justified">
          <p>
            We hypothesize that it would be easier to train action policies with skill instruction annotations compared to directly training a policy for the whole task. Such low-level action policies can then be combined with a high-level planner that generates skill instructions given a task instruction to solve the manipulation task intended by the user. Specifically, the bi-level planner consists of two modules: (1) a high-level task planner and (2) a low-level action policy.</p>

          <p> Below is a image illustrating our bi-level planning framework. The High-Level Task Planner generates a skill instruction as a subgoal for the low-level action policy based on the task instruction and the current observation. Given the subgoal described in the skill instruction, the low-level action policy then generates actions for achieving that subgoal. The high-level task planner updates the skill instruction once every n steps, while the low-level action policy updates the action at every step.</p>
            <figure>
              <img src="./static/images/figure_framework.png"
                   class="image"
                   alt="Part counts among skills" style="max-width: 100%;"/>
            </figure>
        </div>
      </div>
    </div>
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <!-- <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p> -->
          <p>
            Coming soon!
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>Coming Soon</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/partinstruct_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="about:blank" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website was made possible thanks to the incredibly hard work of the team behind the <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
