<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="PartInstruct is concerned with part-level instruction following for fine-grained robot manipulation"
    />
    <meta name="keywords" content="PartInstruct, Manipulate Anything" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      PartInstruct: Part-level Instruction Following for Fine-grained Robot
      Manipulation
    </title>

    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-96FM6LQE7G"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-96FM6LQE7G");
    </script>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/partinstruct.png" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <style>
      /* Apply Fancy Fonts */
      h1,
      h2.title {
        font-family: "Cinzel", serif;
        font-weight: 700;
      }

      h3.subtitle,
      h5.title {
        font-family: "Playfair Display", serif;
        font-weight: 400;
      }
    </style>
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                PartInstruct: Part-level Instruction Following for Fine-grained
                Robot Manipulation
              </h1>
              <div class="is-size-5 publication-authors">
                <a href="https://yifanyin11.github.io" class="author-link">Yifan Yin<span class="upper-mark">*</span>, </a>
                <a href="https://hanzht.com" class="author-link">Zhengtao Han<span class="upper-mark">*</span>, </a>
                <a href="https://www.linkedin.com/in/shivam-aarya/" class="author-link">Shivam Aarya, </a>
                <a href="https://sites.google.com/rice.edu/jianxinjeffwang/home" class="author-link">Jianxin Wang, </a>
                <a href="https://www.linkedin.com/in/shuhang-xu-1805821a5" class="author-link">Shuhang Xu, </a>
                <a href="https://www.linkedin.com/in/jiawei-peng-59a713190/" class="author-link">Jiawei Peng, </a>
                <a href="https://angtianwang.github.io" class="author-link">Angtian Wang, </a>
                <a href="https://www.cs.jhu.edu/~ayuille/" class="author-link">Alan Yuille, </a>
                <a href="https://www.tshu.io/index.html" class="author-link">Tianmin Shu </a>
                <div style="margin-top: 1rem;"></div>
              </div>
              <h2 class="subtitle is-4 publication-authors" style="margin-bottom: 0;">
                Robotics: Science and Systems (RSS) 2025
              </h2>
              <h2 class="subtitle is-5 publication-authors" style="margin-top: 0;">
                (7th Robot Learning Workshop @ ICLR 2025)
              </h2>
              <div class="is-size-5 publication-authors">
                <!-- Publication Links -->
                <div class="column has-text-centered">
                  <div class="publication-links">
                    <span class="link-block">
                      <a
                        href="https://openreview.net/pdf?id=Kb4fDvJBlj"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a
                        href="https://youtu.be/YuwjOJLW-wY?si=1DK8Jr5YQdH6mh_W"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="fab fa-youtube"></i>
                        </span>
                        <span>Video</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a
                        href="https://github.com/SCAI-JHU/PartInstruct"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>
                    <span class="link-block">
                      <a
                        href="https://huggingface.co/datasets/SCAI-JHU/PartInstruct"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="far fa-images"></i>
                        </span>
                        <span>Data</span>
                      </a>
                    </span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img
            src="./static/images/figure_intro.png"
            class="teaser-image"
            alt="PartInstruct teaser image"
          />
          <h2 class="subtitle has-text-centered">
            <span class="dnerf"></span>An example fine-grained robot
            manipulation task in <strong>PartInstruct</strong>
          </h2>
        </div>
      </div>
    </section>

    <!-- Abstract Section -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Fine-grained robot manipulation, such as lifting and rotating a
                bottle to display the label on the cap, requires robust
                reasoning about object parts and their relationships with
                intended tasks. Despite recent advances in training
                general-purpose robot manipulation policies guided by language
                instructions, there is a notable lack of large-scale datasets
                for fine-grained manipulation tasks with part-level instructions
                and diverse 3D object instances annotated with part-level
                labels. We introduce PartInstruct, the first benchmark for
                training and evaluating such models. It features 513 object
                instances across 14 categories, 1302 manipulation tasks in 16
                classes, and over 10,000 expert demonstrations synthesized in a
                3D simulator. Each demonstration includes a high-level task
                instruction, a sequence of basic part-based skills, and
                ground-truth 3D object data. Additionally, we designed a
                comprehensive test suite to evaluate the generalizability of
                learned policies across new states, objects, and tasks. We
                evaluated several state-of-the-art robot manipulation approaches
                including end-to-end vision-language policy learning and
                bi-level planning models on our benchmark. The experimental
                results reveal that current models struggle to robustly ground
                part concepts and predict actions in 3D space, and face
                challenges when manipulating object parts in long-horizon tasks.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Video Carousel -->
    <section class="hero is-light is-small">
      <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item">
              <video
                poster=""
                id="demo_1"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source src="./static/videos/66026_new.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-chair-tp">
              <video
                poster=""
                id="demo_2"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source
                  src="./static/videos/episode1215_frames.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
            <div class="item item-fullbody">
              <video
                poster=""
                id="demo_4"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source src="./static/videos/80332.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-blueshirt">
              <video
                poster=""
                id="demo_5"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source src="./static/videos/95066.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-mask">
              <video
                poster=""
                id="demo_6"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source src="./static/videos/10098.mp4" type="video/mp4" />
              </video>
            </div>
            <div class="item item-coffee">
              <video
                poster=""
                id="demo_7"
                autoplay
                controls
                muted
                loop
                playsinline
                height="100%"
              >
                <source
                  src="./static/videos/episode170165_frames.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Problem Setup -->
    <section class="section">
      <div class="container is-max-desktop">
        <h2 class="title is-3">Problem Setup</h2>
        <div class="youtube-container">
          <iframe 
            width="560" 
            height="315" 
            src="https://www.youtube.com/embed/YuwjOJLW-wY?si=Uw3KvIrhquykFZma" 
            title="YouTube video player" 
            frameborder="0" 
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
            referrerpolicy="strict-origin-when-cross-origin" 
            allowfullscreen>
          </iframe>
        </div>
      </div>
    </section>

    <!-- New Section: PartGym -->
    <section class="section" id="partgym">
      <div class="container is-max-desktop">
        <h2 class="title is-3">
          PartGym: 3D Simulation for Part-level Manipulation
        </h2>
        <div class="content">
          <p>
            <strong>PartGym</strong> is a realistic robot simulator for
            fine-grained manipulation tasks requiring part-level understanding.
            Built on Pybullet, it features a 7-DoF Franka Emika Panda robot with
            a two-finger parallel gripper and simulates manipulation tasks for
            14 types of everyday objects from the PartNet Mobility dataset.
            PartGym provides (1) rich 3D assets, (2) part-level annotations, and
            (3) a diverse task set with natural language instructions. It
            includes 513 object instances and 4,653 part labels for detailed
            manipulation.
          </p>
        </div>

        <!-- Multimodal Observations -->
        <div class="content">
          <p>
            <strong>Multimodal Observations in PartGym</strong>. PartGym
            supports multimodal observations, including RGB images, depth maps,
            scene point clouds (PCDs). It also provides object masks, 2D part
            masks, 3D object PCDs, and 3D part PCDs for each object.
          </p>
          <img
            src="./static/images/figure_vision_modalities_simplified.png"
            class="image"
            alt="Modality figure"
          />
        </div>

        <!-- Object Categories & Object Parts -->
        <div class="content">
          <div class="columns">
            <div class="column">
              <div class="content has-text-centered">
                <h5 class="subtitle is-5 has-text-centered">
                  Object Categories
                </h5>
                <p>
                  The following star plot shows the relative distribution of the
                  number of episodes in each object category in the dataset.
                </p>
                <img
                  src="./static/images/figure_obj_distribution-1.png"
                  class="image"
                  alt="Distribution of object category episodes"
                  style="max-width: 95%"
                />
              </div>
            </div>
            <div class="column">
              <div class="content has-text-centered">
                <h5 class="subtitle is-5 has-text-centered">Object Parts</h5>
                <p>
                  The following graphs show annotated parts grouped by object
                  categories. Spatial part names are highlighted in light gray.
                </p>
                <img
                  src="./static/images/figure_heatmap.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 110%"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- New Section: PartInstruct Benchmark -->
    <section class="section" id="partinstruct">
      <div class="container is-max-desktop">
        <h2 class="title is-3">
          PartInstruct: Benchmark for Part-level Instruction Following
        </h2>

        <!-- Comparison of Benchmarks -->
        <div class="content">
          <p>
            <strong>Comparison of PartInstruct with Other Benchmarks</strong>.
            We compared PartInstruct with existing tabletop robot manipulation
            benchmarks based on: the number of distinctive part-level
            instructions, part labels, part-level tasks, availability of
            training demonstrations, and whether these demonstrations include
            part-level annotations such as 2D and 3D segmentation masks.
          </p>
          <img
            src="./static/images/compare.png"
            class="image"
            alt="Part counts among skills"
            style="max-width: 100%"
          />
        </div>

        <!-- Interactive Demo -->
        <div class="content">
          <h5 class="subtitle is-5">Interactive Demos</h5>
          <p>
            Here we provide interactive demos to animate part-level manipulation tasks. You
            can use the slider to see the observation of every timestep and the corresponding skill instruction.
          </p>
          <!-- Demo Selector -->
          <div class="demo-selector" style="margin-bottom:20px;">
            <label for="demo-selector">Select Demo:</label>
            <select id="demo-selector">
              <option value="demo1" selected>Demo 1</option>
              <option value="demo2">Demo 2</option>
              <option value="demo3">Demo 3</option>
              <option value="demo4">Demo 4</option>
            </select>
          </div>
          <div class="columns is-vcentered interpolation-panel" style="max-width:800px; margin:0 auto;">
            <div class="column interpolation-video-column">
              <div id="interpolation-image-wrapper">Loading...</div>
              <input
                class="slider is-large is-info"
                id="interpolation-slider"
                step="1"
                min="0"
                max="100"
                value="0"
                type="range"
                style="width:780px; margin:20px auto; display:block;" />
            </div>
          </div>
        </div>

        <div class="content" style="display: flex; align-items: flex-start">
          <!-- Left Side: Paragraph -->
          <div style="flex: 1; padding-right: 20px">
            <h5 class="subtitle is-5" style="margin-top: 40px">
              Evaluation Protocol
            </h5>
            <p>
              To systematically evaluate model performance, we designed a
              five-level evaluation protocol, each of which evaluates a policy
              in one type of generalization. Namely, generalizability over
              object initial states (OS), novel object instances (OI), novel
              part combinations in the same task type (TP), novel task
              categories (TC), and novel object categories (OC).
            </p>
          </div>

          <!-- Right Side: Image -->
          <div style="flex: 1; text-align: center">
            <img
              src="./static/images/evaluation.png"
              alt="Evaluation Protocol Table"
              style="max-width: 100%; height: auto; margin-top: 1.5em"
            />
          </div>
        </div>

        <div class="content">
          <!-- First Image -->
          <div style="text-align: center; margin-bottom: 20px">
            <figure style="transform: translateX(0px)">
              <img
                src="./static/images/vis/image_1.png"
                alt="Evaluation Protocol Table"
                style="max-width: 90%"
              />
              <figcaption>Left: Training set. Right: Test 1(OS).</figcaption>
            </figure>
          </div>

          <!-- Second Image -->
          <div style="text-align: center">
            <figure style="transform: translateX(0px)">
              <img
                src="./static/images/vis/image_2.png"
                alt="Evaluation Protocol Table"
                style="max-width: 90%"
              />
              <figcaption>Left: Training set. Right: Test 2(OI).</figcaption>
            </figure>
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <!-- Experiments -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <figure>
                <img
                  src="./static/images/vis/image_3_new.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 92%; height: auto; margin-top: 1.5em"
                />
                <figcaption>Above: Training set. Below: Test 3(TP).</figcaption>
              </figure>

              <figure>
                <img
                  src="./static/images/vis/image_4_new.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 90%; height: auto; margin-top: 1.5em"
                />
                <figcaption>Above: Training set. Below: Test 4(TC).</figcaption>
              </figure>
              <figure>
                <img
                  src="./static/images/vis/image_5.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 85%"
                />
                <figcaption>Left: Training set. Right: Test 5(OC).</figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </section>
    <!-- Remaining Content -->
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Experiments -->
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Experiments</h2>
            <div class="content has-text-justified">
              <p>
                In our benchmark, we evaluate two types of approaches to achieve
                general-purpose robot manipulation: (1) end-to-end policy
                learning that directly maps observation and instruction to
                actions and (2) bi-level planning that first generates
                high-level plans (typically subgoals), then compute and execute
                the low-level action plans to achieve the subgoals.
              </p>
              <figure>
                <img
                  src="./static/images/figure_bar_plot_with_background_new-1.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 95%"
                />
                <figcaption>
                  Success Rates of all baselines. The left group represents
                  end-to-end learning policies, while the right group
                  corresponds to bi-level planning models.
                </figcaption>
              </figure>
            </div>
            <h5 class="subtitle is-5">
              <strong>End-to-End Policy Learning</strong>
            </h5>
            <div class="content has-text-justified">
              <p>
                We evaluate several state-of-the-art end-to-end robot
                manipulation policy learning methods, including Diffusion Policy (DP), 
                3D Diffusion Policy (DP3), Act3D, 3D Diffuser Actor (3D-DA),
                RVT2 and Octo. Note that the original DP and DP3 models do not
                support language inputs. To fit the setup of PartInstruct, we
                use a pre-trained T5 language encoder to get the language
                embedding, then concatenated it with other features as the
                observation condition for the denoising diffusion process.
              </p>
            </div>
            <h5 class="subtitle is-5"><strong>Bi-level Planning</strong></h5>
            <div class="content has-text-justified">
              <p>
                We hypothesize that it would be easier to train policies with
                skill instructions compared to directly training a policy for
                the whole task. Such a
                <strong>low-level action policy</strong> can then be combined
                with a <strong>high-level task planner</strong> that generates
                skill instructions given a task instruction to solve the
                manipulation task. Below is a figure illustrating our bi-level
                planning framework.
              </p>
              <figure>
                <img
                  src="./static/images/figure_framework.png"
                  class="image"
                  alt="Part counts among skills"
                  style="max-width: 105%"
                />
              </figure>
              <p>
                <strong>High-level Task Planner.</strong> We leverage a Vision
                Language Model (VLM) for high-level task planning. At step
                <span>\( t \)</span>, we prompt the VLM with the task
                instruction <span>\( I_\text{task} \)</span> to generate the
                skill instruction for the current step as the subgoal
                <span>\( sg_t \)</span>, i.e.,
                <span>\( \pi_\text{VLM}(sg_t | o_t, I_\text{task}) \)</span>,
                where <span>\( o_t \)</span> is the observation at
                <span>\( t \)</span>.
              </p>
              <p>
                <strong>Low-level Action Policy.</strong> The low-level action
                policy is a vision-language policy that generates manipulation
                actions based on a subgoal and the current observation, i.e.,
                <span>\( \pi(a_t | o_t, sg_t) \)</span>, where
                <span>\( a_t \)</span> is the action at step
                <span>\( t \)</span>. We can train such policies using the skill
                instructions annotated for training demonstrations in our
                dataset. Here, we select the best-performing end-to-end policy
                learning baseline, DP3, to train such policy with object part
                segmentation as part of the input, which we refer to as
                <em>DP3-S</em>.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Qualitative Results Section -->
    <section class="section">
      <div class="container is-max-desktop">
        <h2
          class="title is-3 has-text-weight-semibold"
          style="margin-bottom: 20px"
        >
          Qualitative Results
        </h2>
        
        <p class="content has-text-justified" style="margin-bottom: 25px">
          Rollout visualizations for some baselines are shown in this section.
        </p>
        
        <h5 class="subtitle is-5 has-text-weight-semibold has-text-left" style="margin-bottom: 30px">
          Model Rollouts
        </h5>

        <!-- Video Grid (2x2) -->
        <div
          class="columns is-multiline is-centered"
          style="margin-bottom: 40px"
        >
          <!-- Video 1 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Touch the right of the bottle.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP3</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/test2_2_env_2_video.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>

          <!-- Video 2 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Place the gripper tip on the lid of the pot.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/test3_2_env_1_video.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>

        <div
          class="columns is-multiline is-centered"
          style="margin-bottom: 40px"
        >
          <!-- Video 1 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Grab a handle of the scissors and move it to the left.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/qualitative_results/DP_compressed/compressed_test2_3_env_2_180910.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>

          <!-- Video 2 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Hold the left of the mug, slide back, then release it.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/qualitative_results/DP_compressed/compressed_test3_5_env_5_mug_2.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>

        <div
          class="columns is-multiline is-centered"
          style="margin-bottom: 40px"
        >
          <!-- Video 1 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Push the right side of the kitchen pot to the left.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/qualitative_results/DP_compressed/compressed_test1_4_env_4_video_300401.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>

          <!-- Video 2 -->
          <div class="column is-half">
            <div class="video-container">
              <p
                class="subtitle is-5"
                style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
              >
                <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
                Grab the handle of the kettle.
              </p>
              <div
                class="subtitle-bar"
                style="
                  border-bottom: 3px solid #a9a9a9;
                  width: 100%;
                  margin: 0 0 16px 0;
                "
              ></div>
              <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: Gemini-2.0 Flash+DP3-S</p>
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/test5_1_video_episode_100472.mp4"
                  type="video/mp4"
                />
              </video>
            </div>
          </div>
          <!-- Video 3 -->
          <div class="column is-half">
          <div class="video-container">
            <p
              class="subtitle is-5"
              style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
            >
              <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
              Take the mug on the right, slide it left, then let go.
            </p>
            <div
              class="subtitle-bar"
              style="
                border-bottom: 3px solid #a9a9a9;
                width: 100%;
                margin: 0 0 16px 0;
              "
            ></div>
            <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="./static/videos/test3_5_env_5_mug.mp4" type="video/mp4" />
            </video>
          </div>
        </div>

        <!-- Video 4 -->
        <div class="column is-half">
          <div class="video-container">
            <!-- Add min-height to this subtitle -->
            <p
              class="subtitle is-5"
              style="margin-bottom: 4px; min-height: 60px; display: flex; align-items: center; justify-content: flex-start;"
            >
              <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
              Place gripper tip on the top of the stapler.
            </p>
            <div
              class="subtitle-bar"
              style="
                border-bottom: 3px solid #a9a9a9;
                width: 100%;
                margin: 0 0 16px 0;
              "
            ></div>
            <p style="margin: -12px 0 12px 0; font-size: 1.2rem; color: #666; text-align: left;">MODEL: DP</p>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="./static/videos/qualitative_results/DP_compressed/compressed_test1_2_env_2_video_300250.mp4" type="video/mp4" />
            </video>
          </div>
        </div>
      </div>

      <div class="container is-max-desktop">
        <h5 class="subtitle is-5 has-text-weight-semibold has-text-left" style="margin-bottom: 30px; text-align: left;">
          Comparison of End-to-End and Bi-level Episodes
        </h5>

        <p class="subtitle is-5" style="margin-bottom: 0px; min-height: 50px; display: flex; align-items: center; justify-content: flex-start; text-align: left;">
          <img src="./static/interpolation/Picture1.png" style="width: 24px; height: 32px; margin-right: 8px;">
          Lift the box by its rotation lid, move it to the right, turn the bottom to face front, then set it down.
        </p>

        <!-- Single Bar for Two Videos -->
        <div
          class="comparison-bar"
          style="
            border-bottom: 3px solid #a9a9a9;
            width: 100%;
            margin: 0px 0 15px 0;
          "
        ></div>

        <!-- Video Row -->
        <div class="columns is-centered">
          <!-- Video 1: DP3 -->
          <div class="column is-half">
            <div class="video-container">
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/FAILED_compare_with_VLM_box.mp4"
                  type="video/mp4"
                />
              </video>
              <p
                class="caption has-text-centered"
                style="margin-top: 5px; font-size: 1.2rem; font-weight: 500;"
              >
                MODEL: DP3 (Failure - Incorrect Movement)
              </p>
            </div>
          </div>

          <!-- Video 2: Gemini-2.0 Flash+DP3-S -->
          <div class="column is-half">
            <div class="video-container">
              <video autoplay controls muted loop playsinline width="100%">
                <source
                  src="./static/videos/test4_13_video_box.mp4"
                  type="video/mp4"
                />
              </video>
              <p
                class="caption has-text-centered"
                style="margin-top: 5px; font-size: 1.2rem; font-weight: 500;"
              >
                MODEL: Gemini-2.0 Flash+DP3-S (Success)
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Conclusion Section -->
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Conclusion</h2>
            <div class="content has-text-justified">
              <p>
                In this work, we introduced PartInstruct, a large-scale
                benchmark designed to advance fine-grained robot manipulation
                using part-level instructions. By curating a diverse set of
                objects, tasks, and expert demonstrations, PartInstruct provides
                a foundation for training and evaluating robot manipulation
                models that require reasoning about object parts and their
                relationships with tasks. Our evaluations of state-of-the-art
                models highlight critical challenges in grounding part concepts
                and executing long-horizon tasks. With comprehensive experiments
                and ablation studies, our work provides key insights for future
                research, highlighting the need for further innovation in
                perception, reasoning, and planning to enable robots to
                effectively perform fine-grained, part-aware manipulation.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- BibTeX -->
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{yin2025partinstruct,
          title={PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation},
          author={Yin, Yifan and Han, Zhengtao and Aarya, Shivam and Xu, Shuhang and Wang, Jianxin and Peng, Jiawei and Wang, Angtian and Yuille, Alan and Shu, Tianmin},
          booktitle={Proceedings of Robotics: Science and Systems (RSS)},
          year={2025}
}</code></pre>
      </div>
    </section>
  </body>
</html>
